<section>
<title><m>QR</m> factorization</title>


<introduction>
<p>
Consider a <m>m\times n</m> matrix <m>A</m>; the columns of <m>A</m> can be considered as <m>n</m> vectors in <m>\R^m</m>. If those vectors are linearly independent, they span an <m>n</m>-dimensional subspace of <m>\R^m</m>. The process of Gram-Schmidt orthonormalization performed upon that set of vectors will result in an orthonormal basis for that subspace. Collecting these vectors as the column vectors of a matrix <m>Q</m> gives us the first half of the <m>QR</m> factorization of <m>A</m>. The matrix <m>R</m> is produced as an upper-triangular matrix recording the magnitudes of the projections removed in each iteration of step 2a above, along with the scaling factors of step 2b. That is, <m>R=[r_{i,j}]</m> is a <m>n\times n</m> matrix with
<md>
  <mrow>  r_{i,j} \amp = \begin{cases}\vec{q}_i\cdot \vec{w}_j, \amp  i\lt j \\
	\abs{\vec{w}_j}, \amp  i=j \\
	0, \amp i>j.
\end{cases}</mrow>
</md>
</p>

<p>
This means that the order in which elements of <m>R</m> are recorded during the algorithm will be sort of <q>transpose</q> to the normal order: <m>r_{1,1}</m>, <m>r_{1,2}</m>, <m>r_{2,2}</m>, <m>r_{1,3}</m>, <m>r_{2,3}</m>, <m>r_{3,3}</m>, and so on.
</p>

<p>
The matrices <m>Q</m> and <m>R</m> produced as just described are the <em>reduced <m>QR</m> factorization of <m>A</m></em>. If it exists, it is customary to add <m>m-n</m> extra columns to <m>A</m>; these can be any column vectors linearly independent from the original columns of <m>A</m>, so it is often the case that the columns added correspond to vectors <m>\vc{1,0,0,\ldots}</m>, <m>\vc{0,1,0,\ldots}</m>, <m>\vc{0,0,1,\ldots}</m>, and so on. The orthonormal basis will then be a full basis for <m>\R^m</m> containing <m>m</m> vectors; in order that <m>A=QR</m>, it must be that <m>R</m> is a <m>m\times n</m> matrix. In fact this is the case, with <m>R=[r_{i,j}]</m> the <m>m\times n</m> matrix with the same formula from <xref ref="eq_rmatrix" autoname="yes" /> determining its elements. This different factorization is called the <em>full <m>QR</m> factorization of <m>A</m></em>.
</p>
<definition>
<statement>
<p>
A square matrix <m>Q</m> is <term>orthogonal</term> if and only if <m>Q^T=Q^{-1}</m>.
</p>
</statement>
</definition>

<theorem>
<statement>
<p>
Suppose <m>QR=A</m> is the full <m>QR</m> factorization of <m>A</m>. Then <m>Q</m> is an orthogonal matrix.
</p>
</statement>
</theorem>

<proof>
<p>
Consider <m>Q^TQ = [m_{i,j}]</m>. Then
<me>
  m_{i,j} = \vec{q}_i\cdot\vec{q}_j = \begin{cases}0\amp i\neq j\\1\amp i=j,
\end{cases}
</me>
and the desired result holds.
</p>
</proof>
</introduction>


<subsection>
<title>Utility of <m>QR</m> factorization</title>
<p>
While we can use the <m>QR</m> factorization to solve a system of equations, it is approximately three times more complicated than <m>LU</m> decomposition. However, <m>QR</m> decomposition can avoid a problem called <em>ill-conditioning</em> in solving the least squares problem.
</p>
<algorithm>
<title>Least squares by $QR$ factorization</title>
<p>
Given the <m>m\times n</m> inconsistent system of equations represented by <m>A\vec{x}=\vec{b}</m>, let <m>QR=A</m> be the full <m>QR</m> factorization of <m>A</m> and let
<md>
  <mrow>  \hat{R} \amp = \text{ upper \(n\times n\) submatrix of \(R\) }</mrow>
  <mrow>  \hat{\vec{d}} \amp = \text{ upper \(n\) entries of } \vec{d}=Q^T\vec{b}</mrow>
</md>
</p>

<p>
Then the solution to <m>\hat{R}\vec{\bar{x}}=\hat{\vec{d}}</m> is the least squares solution <m>\bar{\vec{x}}</m>.
</p>
</algorithm>
</subsection>

</section>
