<section>
<title>Gram-Schmidt orthonormalization</title>
<p>
A first method for producing an orthogonal matrix from an arbitrary matrix is the Gram-Schmidt algorithm. In essence, the idea is to treat each column of the initial matrix as a vector, and proceed through in order orthogonalizing each vector to those which have previously been orthonormalized.
</p>

<example>
<statement>
<p>
Suppose we begin with two vectors,
<md>
  <mrow>  \vec{x}_1 \amp = \vc{1,1,1,1,1}</mrow>
  <mrow>  \vec{x}_2 \amp = \vc{-2,-1,0,1,3},</mrow>
</md>
both in <m>\R^5</m>. Then we can imagine somehow in <m>\R^5</m> that these two vectors uniquely determine a plane <mdash /> if we let the ``positive horizontal axis" in that plane be determined by the direction of <m>\vec{x}_1</m> and the ``positive vertical axis" by <m>\vec{x_2}</m>, we have to determine the projection of <m>\vec{x}_2</m> onto <m>\vec{x}_1</m>.
</p>
<figure xml:id="fig_proj2d" >
<caption>Two vectors and the plane they form.</caption>
<image>
<latex-image-code><![CDATA[\begin{tikzpicture}\draw [->] (3,0) -- (6,0) node [right] {\(\vec{x}_1\)};
\draw [->] (0,0) -- (3,4) node [above right] {\(\vec{x}_2\)};
\draw [->] (0,0) -- node [below] {\(\textrm{proj} _{\vec{x}_1}\vec{x}_2\)} (3,0);
\draw [dashed, ->] (0,0) -- node [left] {\(\vec{x}_2-\textrm{proj} _{\vec{x}_1}\vec{x}_2\)} (0,4);
\draw [dashed] (3,0) -- (3,4);\end{tikzpicture}]]></latex-image-code>
</image>

</figure>
<p>
Specifically, we want the ``vertical axis" to be in the direction of <m>\vec{x}_2-\textrm{proj} _{\vec{x}_1}\vec{x}_2</m>, the component of <m>\vec{x}_2</m> orthogonal to the projection of <m>\vec{x}_2</m> onto <m>\vec{x}_1</m>, <m>\textrm{proj} _{\vec{x}_1}\vec{x}_2</m>. For an illustration, see <xref ref="fig_proj2d" autoname="yes" />.
</p>
</statement>
</example>

<p>
What is very intuitive about the Gram-Schmidt process is how it extends: to add a third vector <m>\vec{x}_3</m> and orthonormalize it, we subtract from <m>\vec{x}_3</m> its projections onto the two previously-determined ``axes." To add a fourth, we subtract its projection onto each of three previously determined axes. The process continues iterating through the <m>n</m> vectors which form the columns of matrix <m>X</m>.
</p>

<p>
The formula for the projection of a vector <m>\vec{b}</m> onto a vector <m>\vec{a}</m> follows from the correspondence between the dot product and cosine of the interior angle of two vectors: <m>\vec{a}\cdot\vec{b} = \abs{\vec{a}}\abs{\vec{b}}\cos\alpha</m>, where <m>0\leq \alpha\leq \pi</m> is the angle between <m>\vec{a}</m> and <m>\vec{b}</m>. Thus
<me>
  \proj_{\vec{a}}\vec{b} 
	= \left(\abs{\vec{b}}\cos\alpha\right)\frac{\vec{a}}{\abs{\vec{a}}}
	= \left(\frac{\vec{a}\cdot\vec{b}}{\vec{\abs{a}}}\right)\frac{\vec{a}}{\abs{\vec{a}}}.
</me>
</p>

<p>
If it so happens that <m>\abs{\vec{a}}=1</m>, then <m>\vec{a}</m> is a <em>unit vector</em>, and <m>\proj_{\vec{a}}\vec{b} = (\vec{a}\cdot\vec{b})\vec{a}</m>.
</p>

<theorem>
<title>Gram-Schmidt Orthonormalization</title>
<statement>
<p>
Suppose <m>S=\set{\vec{x}_1,\vec{x}_2,\cdots,\vec{x}_n}</m> is a set of vectors in <m>\R^m</m>. Suppose
<md>
  <mrow>  \vec{w}_1 \amp = \vec{x}_1,\text{ and }</mrow>
  <mrow>  \vec{q}_1 \amp = \vec{w}_1/\abs{\vec{w}_1}.</mrow>
</md>
</p>

<p>
Then for each <m>i=2,3,\ldots,n</m>, let
<md>
  <mrow>  \vec{w}_i \amp = \vec{x}_i-\sum_{j=1}^{i-1} (\vec{q}_j\cdot\vec{x}_i)\vec{q}_j,\text{ and }</mrow>
  <mrow>  \vec{q}_i \amp = \vec{w}_i/\abs{\vec{w}_i}.</mrow>
</md>
</p>

<p>
Then <m>Q=\set{\vec{q}_1,\vec{q}_2,\ldots,\vec{q}_n}</m> is an orthonormal basis for <m>\lspan(S)</m>.
</p>
</statement>
</theorem>

<proof>
<p>
Suppose <m>1\leq k\lt \ell\leq n</m>. Then
<md>
  <mrow>  \vec{q}_k\cdot\vec{w}_{\ell} \amp = \vec{q}_k\cdot\vec{x}_{\ell} - \vec{q}_k\cdot\sum_{j=1}^{\ell-1} (\vec{q}_j\cdot\vec{x}_\ell)\vec{q}_j</mrow>
  <mrow>  \amp = \vec{q}_k\cdot\vec{x}_{\ell} - \sum_{j=1}^{\ell-1}(\vec{q}_j\cdot \vec{x}_\ell)(\vec{q}_k\cdot\vec{q}_j)</mrow>
</md>
</p>

<p>
Since
<md>
  <mrow>  \vec{q}_k\cdot\vec{q}_j \amp = \begin{cases}0\amp k\neq j\\1\amp k=j,
\end{cases}</mrow>
</md>
we have <m>\vec{q}_k\cdot\vec{w}_\ell = \vec{q}_k\cdot\vec{x}_\ell - \vec{q}_k\cdot\vec{x}_\ell = 0</m>. Thus <m>\vec{q}_k\cdot\vec{q}_\ell = \frac1{\abs{\vec{w}_\ell}}(\vec{q}_k\cdot\vec{w}_\ell) = 0</m> and <m>\vec{q}_k</m> is orthogonal to <m>\vec{q}_{\ell}</m> whenever <m>k\lt \ell</m>. Moreover,
<me>
  \abs{\vec{q}_k} = \abs{\frac{\vec{w}_k}{\abs{\vec{w}_k}}} = 1,
</me>
so <m>Q</m> is a set of mutually orthogonal unit vectors. Since <m>\vec{w}_\ell=\abs{\vec{w}_\ell}\vec{q}_\ell</m>, each vector <m>\vec{x}_\ell</m> can be written as a linear combination from <m>\set{\vec{q}_k:k\leq\ell}</m>; hence any linear combination of vectors in <m>S</m> can be written as a linear combination of vectors in <m>Q</m>, and vice versa. Thus <m>\lspan S=\lspan Q</m> and the result holds.
</p>
</proof>

<p>
The Gram-Schmidt algorithm actually follows directly from the statement of the theorem and the described process, with one minor modification to improve the numerical stability of the algorithm.
</p>
<algorithm>
<title>Modified Gram-Schmidt Orthonormalization</title>
<p>
Let
<me>
  S=\set{\vec{x}_j:1\leq j\leq n}
</me>
be a set of vectors in <m>\R^m</m>.
</p>

<ol>
<li><p>For each <m>j</m> in <m>\set{1,2,\ldots,n}</m>, set <m>\vec{w}_j = \vec{x}_j</m>. Some of these will be updated later.</p></li>
<li><p>For <m>j</m> in <m>\set{1,2,\ldots,n}</m>, do the following.
</p>

<ol>
<li><p>For each <m>i\lt j</m>, set <m>\vec{w}_j = \vec{w}_j - (\vec{q_i}\cdot \vec{w}_j)\vec{q}_i</m>.</p></li>
<li><p>Let <m>\vec{q}_j = \frac1{\abs{\vec{w}_j}}\vec{w}_j</m>.</p></li>
</ol>
</li>
</ol>

<p>
The set <m>Q=\set{\vec{q}_j:1\leq j\leq n}</m> is an orthonormal basis for <m>\lspan(S)</m>.
</p>
</algorithm>
</section>
