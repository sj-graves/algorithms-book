<section xml:id="sec-QRdecomp-LeastSquares">
    <title>Least squares regression</title>


    <introduction>
        <p>
            The least squares problem goes back to the beginnings of matrix theory, and in fact to the very problems which Gauss and Legendre considered during the early 1800s.
            Consider the following scatter plot:
        </p>
        <figure xml:id="fig_scatter" >
            <image source="./images/scatter.svg" />
            <caption>Points approximately following a parabola.</caption>
        </figure>
        <p>
            It appears that the points lie along some sort of parabola, but not <em>exactly</em> along the parabola <mdash /> there seems to be some error involved in the fit of the curve. A question one might naturally ask is, <q>What is the equation of the parabola which best fits the given data?</q> This and related questions are most often answered by the process of <em>least squares regression.</em>
        </p>
    </introduction>


    <subsection>
        <title>Inconsistent systems of linear equations</title>
        <p>
            Consider the system
            <men xml:id="eq_incons">
                \left\{\begin{aligned}
                3x_1 - 2x_2 \amp = 4, \\
                2x_1 +  x_2 \amp = 9, \\
                -3x_1 + 2x_2 \amp = 6.
                \end{aligned}\right.
            </men>
            There is a problem with this system which will not appear with only a passing glance <mdash /> the first and third equations cannot both hold for the same vector <m>\vc{x_1,x_2}</m>.
        </p>
        <definition>
            <statement>
                An <term>inconsistent system of equations</term> in <m>n</m> variables is a system of equations such that no vector <m>\vc{x_1,x_2,\ldots,x_n}</m> simultaneously satisfies all of the equations in the system.
            </statement>
        </definition>

        <!--p>
            It should be immediately clear that this is an inconsistent system, since not both the first and the third equations can be simultaneously valid. Hence there is no <m>\vec{x}=\vc{x_1,x_2}</m> which satisfies the system. Is it possible then to find some <m>\bar{x}</m> which is the ``best" approximate solution?
        </p-->

        <p>
            We can recast <xref ref="eq_incons" autoname="yes" /> as a vector and matrix problem by writing it as <m>x_1\vec{v}_1+x_2\vec{v}_2 = A\vec{x} = \vec{b}</m> where
            <md>
                <mrow>  x_1\vec{v_1}+x_2\vec{v_2}   
                    \amp = x_1\begin{bmatrix}3</mrow>
                <mrow>  2</mrow>
                <mrow>  -3\end{bmatrix} + x_2\begin{bmatrix}-2</mrow>
                <mrow>  1</mrow>
                <mrow>  2\end{bmatrix}
                    = \begin{bmatrix}3\amp -2</mrow>
                <mrow>  2\amp 1</mrow>
                <mrow>  -3\amp 2\end{bmatrix} \begin{bmatrix}x_1</mrow>
                <mrow>  x_2\end{bmatrix} 
                    =\begin{bmatrix}4</mrow>
                <mrow>  9</mrow>
                <mrow>  6\end{bmatrix}=\vec{b}.</mrow>
            </md>
            In fact, we have now written the problem in terms of a linear combination <m>x_1\vec{v_1}+x_2\vec{v_2}</m>, but the vector <m>\vec{b}</m> can actually not validly be written as a linear combination of those two vectors!
        </p>
        <definition>
            <statement>
                <p>
                    A vector <m>\vec{w}</m> is a <term>linear combination</term> from a set <m>S=\set{\vec{v}_1,\vec{v}_2,\ldots,\vec{v}_k}</m> if and only if there are scalars <m>a_1</m>, <m>a_2</m>, <ellipsis />, <m>a_k</m> such that
                    <me>
                        \vec{w} = a_1\vec{v}_1+a_2\vec{v}_2+\cdots+a_k\vec{v}_k.
                    </me>
                </p>
                <p>
                    The set of all such vectors <m>\vec{w}</m> is the <term>span of <m>S</m></term>. The set <m>S</m> is <term>linearly independent</term> if and only if no vector <m>\vec{v}\in S</m> can be written as a linear combination of the vectors in <m>S\setminus\set{\vec{v}}</m>; equivalently, <m>S</m> is linearly independent if 
                    <me>
                        a_1\vec{v}_1+a_2\vec{v}_2+\cdots+a_k\vec{v}_k = \vec{0}
                    </me>
                    only when <m>a_1=a_2=\cdots=a_k=0</m>.
                </p>
            </statement>
        </definition>
        <p>
            This allows us to recast our problem in linear algebra terminology <mdash /> we need to find the vector <m>\barx{x}</m> in the span of <m>\set{\vec{v}_1,\vec{v}_2}</m> which is nearest to <m>\vec{b}</m>. Since we are working with real coefficients, it makes perfect sense to assume that the vector space in which we are working is <m>\R^n</m>, and this allows us to appeal to our knowledge of Euclidean geometry.
            <!--A convenient fact of working over the real numbers is that we already know how to do this: the shortest path from a line <m>\ell</m> to a point <m>P</m> not on the line is the vector <m>\vec{r}</m> from <m>X\in\ell</m> to <m>P</m> where the line <m>\overline{XP}</m> is perpendicular to <m>\ell</m>. To extend this idea to higher dimensions, the vector <m>\vec{r}</m> must be <em>orthogonal</em> to any other vector <m>\vec{v}</m> in the linear subspace<fn>Line, plane, or hyperplane!</fn>.-->
        </p>

        <theorem>
            <statement>
                The shortest path from a line <m>\ell</m> to a point <m>P</m> not on <m>\ell</m> is the vector <m>\vec{r}</m> with tail at a point <m>X\in \ell</m> and head <m>P</m>, such that the line <m>\overline{XP}</m> is perpendicular to <m>\ell</m>.
            </statement>
        </theorem>
        <p>
            This theorem extends naturally to higher dimensions: the shortest path between a <em>linear subspace</em> <m>H</m> of the vector space and a point <m>P\notin H</m> is along a vector which is orthogonal to every vector <m>\vec{h}\in H</m>. The use of the term <em>orthogonal</em> indicates that these ideas extend beyond regular Euclidean geometry.
        </p>
        <definition>
            <statement>
                <p> Let <m>\vec{u}, \vec{v}\in\R^n</m>. The <term>dot product of <m>\vec{u}</m> and <m>\vec{v}</m></term> is the scalar quantity
                    <me>\vec{u}\cdot\vec{v} = u_1v_1+u_2v_2+\cdots+u_nv_n.</me> The vectors <m>\vec{u}</m> and <m>\vec{v}</m> are <term>orthogonal</term> if and only if <m>\vec{u}\cdot\vec{v}=0</m>.
                </p>
            </statement>
        </definition>
        <lemma>
            <statement>
                <p>
                    If the vectors <m>\vec{u},\vec{v}\in\R^n</m> are considered as <m>n\times 1</m> matrices, then <m>\vec{u}\cdot\vec{v}=\vec{u}^T\vec{v}</m>, where <m>\vec{u}^T</m> is the matrix transpose of <m>\vec{u}</m>.
                </p>
            </statement>
        </lemma>

        <p>
            An easy fact to prove is that the only vector in a space which is orthogonal to every vector in its space is the zero vector.
        </p>

        <theorem>
            <statement>
                <p>
                    Suppose <m>\vec{u}_0\in\R^n</m>. If <m>\vec{u}_0\cdot\vec{v}=0</m> for every <m>\vec{v}\in\R^n</m>, then <m>\vec{u}_0 = \vec{0}</m>.
                </p>
            </statement>
        </theorem>

        <proof>
            <p>
                Suppose <m>\vec{v}\in\R^n</m> is an arbitrary vector. Then
                <me>
                    \vec{0}\cdot \vec{v} = \sum_{i=1}^n 0v_i = 0,
                </me>
                so <m>\vec{0}</m> is orthogonal to every vector in <m>\R^n</m>. Now suppose <m>\vec{u}\neq \vec{0}</m>; then at least one of the components of <m>\vec{u}</m> is nonzero; suppose that <m>u_k\neq 0</m>. Then
                <me>
                    \vec{u}\cdot\vec{u} = \sum_{i=0}^n u_i^2 \geq u_k^2 > 0.
                </me>
            </p>

            <p>
                Hence <m>\vec{u}</m> is not orthogonal to itself, and therefore there is a nonzero vector in <m>\R^n</m> to which <m>\vec{u}</m> is not orthogonal.
            </p>
        </proof>

        <p>
            The next theorem establishes that if a vector <m>\vec{b}</m> is not in the span of a set <m>S</m> of vectors, then there is a unique vector in <m>\lspan(S)</m> which is the <em>closest approximation to <m>\vec{b}</m> in the span of <m>S</m></em>. As the proof is complicated, it is omitted.
        </p>

        <theorem>
            <statement>
                <p>
                    Suppose <m>S=\set{\vec{v}_1,\vec{v}_2,\ldots,\vec{v}_k}</m> is a set of linearly independent vectors in <m>\R^n</m> for <m>n>k</m> and <m>\vec{b}\in\R^n\setminus\lspan(S)</m>. Then there is a unique vector <m>\barx{u}\in\lspan(S)</m> such that the magnitude of <m>\vec{r}=\vec{b}-\barx{u}</m> is minimum and <m>\vec{r}</m> is orthogonal to every vector <m>\vec{u}\in\lspan(S)</m>.
                </p>
            </statement>
        </theorem>
        <p>
            The vector <m>\barx{u}</m> given in the preceding theorem is the approximation to <m>\vec{b}</m> in the span of <m>S</m>.
        </p>

        <p>
            Looking back to <xref ref="eq_incons" autoname="yes"/> we had determined vectors <m>\vec{v}_1 = \vc{3,2,-3}</m> and <m>\vec{v}_2=\vc{-2,1,2}</m>, and a vector <m>\vec{b}=\vc{4,9,6}\notin\lspan\set{\vec{v}_1,\vec{v}_2}</m> such that the system was equivalent to writing
            <md>
                <mrow>
                    x_1\vec{v}_1 + x_2\vec{v}_2 \amp = \vec{b} 
                </mrow><intertext>or</intertext>
                <mrow>
                    A\vec{x} \amp = \vec{b}
                </mrow>
                <intertext>where</intertext>
                <mrow>
                    A = 
                    \begin{bmatrix}
                    \vec{v}_1 \amp \vec{v}_2 
                    \end{bmatrix} \amp = 
                    \begin{bmatrix}
                    3\amp -2\\2\amp 1\\-3\amp 2
                    \end{bmatrix}.
                </mrow>
            </md>
            However, it is also the case that <m>\vec{b}\notin\lspan\set{\vec{v}_1\vec{v}_2}</m>.
        </p>
        <p>
            A vector <m>\vec{u}</m> is in the span of <m>\set{\vec{v}_1,\vec{v}_2}</m> if and only if there is some <m>\vec{x}\in\R^2</m> such that <m>A\vec{x}=\vec{u}</m>.  If we want to find <m>\barx{u}=A\barx{x}</m> such that <m>\vec{r}=\vec{b}-A\barx{x}</m> is minimized, it suffices to find <m>\barx{x}</m> such that <m>(A\vec{x})\cdot(\vec{b}-A\barx{x})=0</m> for every <m>\vec{x}\in\R^2</m>.
        </p>
        <p>
            Supposing such a <m>\vec{\bar{x}}</m> exists,
            <md>
                <mrow>  0 = (A\vec{x})\cdot(\vec{b}-A\barx{x}) \amp = (A\vec{x})^T(\vec{b}-A\barx{x})</mrow>
                <mrow>  \amp = \vec{x}^TA^T(\vec{b}-A\barx{x})</mrow>
                <mrow> \amp = \vec{x}^T\left( A^T\vec{b}-A^TA\barx{x} \right)</mrow>
            </md>
            for every <m>\vec{x}\in\R^2</m>; but by Theorem 8.3 above, this means that for such a <m>\barx{x}</m> to exist it suffices that <m>A^T\vec{b}-A^TA\barx{x}=\vec{0}</m>, or equivalently that <m>(A^TA)\barx{x} = A^T\vec{b}</m>. This vector equation has a unique solution exactly when <m>A^TA</m> can be reduced to the identity matrix via Gauss-Jordan elimination. The system of equations corresponding to the equation <m>(A^TA)\barx{x} = A^T\vec{b}</m> are called the <em>normal equations for least squares</em>.
        </p>

        <theorem>
            <title>Normal equations for least squares</title>
            <statement>
                <p>
                    Suppose <m>A</m> is a <m>m\times n</m> real matrix and <m>\vec{b}\in\R^m</m>. The inconsistent system of equations represented by the matrix equation
                    <me>
                        A\vec{x}=\vec{b}
                    </me>
                    has a <term>least squares approximation <m>\barx{x}</m></term> determined by the solution of the <term>normal equations</term>,
                    <me>
                        (A^TA)\barx{x}=A^T\vec{b}.
                    </me>
                </p>

                <p>
                    The <term>residual vector <m>\vec{r}=\vec{b}-A\barx{x}</m></term> is the vector of minimum magnitude in the set
                    <me>
                        \set{\vec{b}-A\vec{x}:\vec{x}\in\R^n}.
                    </me>
                </p>
            </statement>
        </theorem>
        
        <p>
            We close this section simply by using Sage to determine the solution to <xref ref="eq_incons" autoname="yes" />.
        </p>
        <sage>
            <input>
                A = matrix([[3,-2],[2,1],[-3,2]])
                AT = A.transpose()
                b = vector([4,9,6])
                show((AT*A).inverse() * AT * b)
            </input>
        </sage>
    </subsection>

</section>
