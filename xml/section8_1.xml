<section>
<title>Least squares regression</title>


<introduction>
<p>
The least squares problem goes back to the beginnings of matrix theory, and in fact to the very problems which Gauss and Legendre considered during the early 1800s.
Consider the scatter plot in <xref ref="fig_scatter" autoname="yes" />.
</p>
<figure xml:id="fig_scatter" >
<caption>A scatter plot.</caption>
<image>
<latex-image-code><![CDATA[\begin{tikzpicture}[yscale=1/6, xscale=5/6]
\foreach \p in {(-5, 26.18), (-24/5, 24.22), (-23/5, 22.94), (-22/5, 20.55), (-21/5, 19.34), (-4, 17.24), (-19/5, 15.69), (-18/5, 14.17), (-17/5, 12.6), (-16/5, 11.25), (-3, 9.96), (-14/5, 8.78), (-13/5, 7.5), (-12/5, 6.67), (-11/5, 5.78), (-2, 4.34), (-9/5, 3.91), (-8/5, 3.33), (-7/5, 2.09), (-6/5, 2.11), (-1, 1.98), (-4/5, 1.46), (-3/5, 1.18), (-2/5, 1.15), (-1/5, 0.95), (0, 1.0), (1/5, 1.07), (2/5, 1.55), (3/5, 1.43), (4/5, 2.05), (1, 2.3), (6/5, 2.83), (7/5, 3.13), (8/5, 4.31), (9/5, 4.4), (2, 5.59), (11/5, 6.42), (12/5, 6.94), (13/5, 8.23), (14/5, 8.85), (3, 10.13), (16/5, 11.19), (17/5, 12.37), (18/5, 13.64), (19/5, 15.03), (4, 16.64), (21/5, 18.48), (22/5, 19.44), (23/5, 21.53), (24/5, 23.59), (5, 25.49)}{
	\draw \p node {\color{blue}\(\bullet\)};
};
\draw [thick] (-5,0) -- (5,0)
		(0,0) -- (0,30);
\foreach \x in {-5,-4,-3,-2,-1,1,2,3,4,5}{
	\draw [thick] (\x,-1) node [below] {\(\x\)} -- (\x,1);
	\draw [dashed] (\x,1) -- (\x,30);
}
\foreach \y in {5,10,15,20,25,30}{
	\draw [thick] (-.2,\y) node [left] {\(\y\)} -- (.2,\y);
	\draw [dashed] (-5,\y) -- (-.2,\y) (.2,\y) -- (5,\y);
}\end{tikzpicture}]]></latex-image-code>
</image>

</figure>
<p>
It appears that the points lie along some sort of parabola, but not exactly along the parabola <mdash /> there seems to be some error involved in the fit of the curve. A question one might naturally ask is, ``What is the equation of the parabola which best fits the given data?"
</p>

<p>
This and related questions are most often answered by the process of <em>least squares regression.</em> We'll approach this problem by a seemingly unrelated means.
</p>
</introduction>


<subsection>
<title>Inconsistent systems of equations</title>
<p>
Consider the system
<md>
  <mrow>  \systeme{3x_1_2x_2=4, 2x_1+x_2=9, -3x_1+2x_2=6.}</mrow>
</md>
</p>

<p>
It should be immediately clear that this is an inconsistent system, since not both the first and the third equations can be simultaneously valid. Hence there is no <m>\vec{x}=\vc{x_1,x_2}</m> which satisfies the system. Is it possible then to find some <m>\bar{x}</m> which is the ``best" approximate solution?
</p>

<p>
We can recast the system <xref ref="eq_incons" autoname="yes" /> as a vector and matrix problem (sadly which is still inconsistent) by writing it as <m>x_1\vec{v}_1+x_2\vec{v}_2 = A\vec{x} = \vec{b}</m> where
<md>
  <mrow>  x_1\vec{v_1}+x_2\vec{v_2}   
	\amp = x_1\begin{bmatrix}3</mrow>
  <mrow>  2</mrow>
  <mrow>  -3\end{bmatrix} + x_2\begin{bmatrix}-2</mrow>
  <mrow>  1</mrow>
  <mrow>  2\end{bmatrix}
	= \begin{bmatrix}3\amp -2</mrow>
  <mrow>  2\amp 1</mrow>
  <mrow>  -3\amp 2\end{bmatrix} \begin{bmatrix}x_1</mrow>
  <mrow>  x_2\end{bmatrix} 
	=\begin{bmatrix}4</mrow>
  <mrow>  9</mrow>
  <mrow>  6\end{bmatrix}=\vec{b}.</mrow>
</md>
</p>

<p>
What's interesting about this way of writing the system is that now the left-hand side is a <em>linear combination</em> of vectors <m>\vec{v}_1</m> and <m>\vec{v}_2</m> and the right-hand side is a vector <m>\vec{b}</m> which <em>can not</em> be written as a linear combination of <m>\vec{v}_1</m> and <m>\vec{v}_2</m>.
</p>
<definition>
<statement>
<p>
A vector <m>\vec{w}</m> is a <term>linear combination</term> from a set <m>S=\set{\vec{v}_1,\vec{v}_2,\ldots,\vec{v}_k}</m> if and only if there is a set of scalars <m>\set{a_1, a_2, \ldots, a_k}</m> such that
<me>
  \vec{w} = a_1\vec{v}_1+a_2\vec{v}_2+\cdots+a_k\vec{v}_k.
</me>
</p>

<p>
The set of all such vectors <m>\vec{w}</m> is the <term>span of <m>S</m></term>. The set <m>S</m> is <term>linearly independent</term> if and only if no vector <m>\vec{v}\in S</m> can be written as a linear combination of the vectors in <m>S\setminus\set{\vec{v}}</m>.
</p>
</statement>
</definition>
<p>
So now we can recast our problem in linear algebra terminology: we need to find the vector <m>\bar{x}\in\lspan\set{\vec{v}_1,\vec{v}_2}</m> which is nearest to <m>\vec{b}</m>. A convenient fact of working over the real numbers is that we already know how to do this: the shortest path from a line <m>\ell</m> to a point <m>P</m> not on the line is the vector <m>\vec{r}</m> from <m>X\in\ell</m> to <m>P</m> where the line <m>\overline{XP}</m> is perpendicular to <m>\ell</m>. To extend this idea to higher dimensions, the vector <m>\vec{r}</m> must be <em>orthogonal</em> to any other vector <m>\vec{v}</m> in the linear subspace<fn>Line, plane, or hyperplane!</fn>.
</p>
<definition>
<statement>
<p>
Two vectors <m>\vec{u}, \vec{v}\in\R^n</m> are <term>orthogonal</term> if and only if <m>\vec{u}\cdot\vec{v}=0</m>.
</p>
</statement>
</definition>
<p>
An easy fact to prove is that the only vector in a space which is orthogonal to every vector in its space is the zero vector.
</p>

<theorem>
<statement>
<p>
Suppose <m>\vec{u}_0\in\R^n</m>. If <m>\vec{u}_0\cdot\vec{v}=0</m> for every <m>\vec{v}\in\R^n</m>, then <m>\vec{u}_0 = \vec{0}</m>.
</p>
</statement>
</theorem>

<proof>
<p>
Suppose <m>\vec{v}\in\R^n</m> is an arbitrary vector. Then
<me>
  \vec{0}\cdot \vec{v} = \sum_{i=1}^n 0v_i = 0,
</me>
so <m>\vec{0}</m> is orthogonal to every vector in <m>\R^n</m>. Now suppose <m>\vec{u}\neq \vec{0}</m>; then at least one of the components of <m>\vec{u}</m> is nonzero; suppose that <m>u_k\neq 0</m>. Then
<me>
  \vec{u}\cdot\vec{u} = \sum_{i=0}^n u_i^2 \geq u_k^2 > 0.
</me>
</p>

<p>
Hence <m>\vec{u}</m> is not orthogonal to itself, and therefore there is a nonzero vector in <m>\R^n</m> to which <m>\vec{u}</m> is not orthogonal.
</p>
</proof>

<p>
The next theorem establishes that if <m>\vec{b}\notin\lspan(S)</m>, then there is some vector in <m>\lspan(S)</m> which is the <em>closest approximation</em> of <m>\vec{b}</m> in <m>\lspan(S)</m>. As the proof is more complicated, it is omitted.
</p>

<theorem>
<statement>
<p>
Suppose <m>S=\set{\vec{v}_1,\vec{v}_2,\ldots,\vec{v}_k}</m> is a set of linearly independent vectors in <m>\R^n</m> for <m>n>k</m> and <m>\vec{b}\in\R^n\setminus\lspan(S)</m>. Then there is a unique vector <m>\vec{\bar{u}}\in\lspan(S)</m> such that the magnitude of <m>\vec{r}=\vec{b}-\vec{\bar{u}}</m> is minimum and <m>\vec{r}</m> is orthogonal to every vector <m>\vec{u}\in\lspan(S)</m>.
</p>
</statement>
</theorem>

<p>
Back to our problem, we're looking at vectors <m>\vec{v}_1 = \vc{3,2,-3}</m> and <m>\vec{v}_2=\vc{-2,1,2}</m>, and a vector <m>\vec{b}=\vc{4,9,6}\notin\lspan\set{\vec{v}_1,\vec{v}_2}</m>. If we let
<me>
  A = \begin{bmatrix}3\amp -2\\2\amp 1\\-3\amp 2
\end{bmatrix} ,
</me>
we see that <m>\vec{u}\in\lspan\set{\vec{v}_1,\vec{v}_2}</m> if and only if there is some <m>\vec{x}=\vc{x_1,x_2}\in\R^2</m> such that <m>A\vec{x}=\vec{u}</m>. If we want to find <m>\vec{\bar{u}}=A\vec{\bar{x}}</m> such that <m>\vec{r}=\vec{b}-A\vec{\bar{x}}</m> is minimized, it suffices to find <m>\vec{\bar{x}}</m> such that <m>(A\vec{x})\cdot(\vec{b}-A\vec{\bar{x}})=0</m> for all <m>\vec{x}\in\R^2</m>.
</p>

<lemma>
<statement>
<p>
If the vectors <m>\vec{u},\vec{v}\in\R^n</m> are considered as <m>n\times 1</m> matrices, then <m>\vec{u}\cdot\vec{v}=\vec{u}^T\vec{v}</m>.
</p>
</statement>
</lemma>

<p>
Supposing such a <m>\vec{\bar{x}}</m> exists,
<md>
  <mrow>  0 = (A\vec{x})\cdot(\vec{b}-A\vec{\bar{x}}) \amp = (A\vec{x})^T(\vec{b}-A\vec{\bar{x}})</mrow>
  <mrow>  \amp = \vec{x}^TA^T(\vec{b}-A\vec{\bar{x}})</mrow>
</md>
for all <m>\vec{x}\in\R^2</m>; but by Theorem 8.3 above, this means that for such a <m>\vec{\bar{x}}</m> to exist it suffices that <m>A^T\vec{b}-A^TA\vec{\bar{x}}=\vec{0}</m>, or equivalently that <m>(A^TA)\vec{\bar{x}} = A^T\vec{b}</m>. This vector equation has a unique solution exactly when <m>A^TA</m> can be reduced to the identity matrix via Gauss-Jordan elimination. The system of equations corresponding to the equation <m>(A^TA)\vec{\bar{x}} = A^T\vec{b}</m> are called the <em>normal equations for least squares</em>.
</p>

<theorem>
<title>Normal equations for least squares</title>
<statement>
<p>
Suppose <m>A</m> is a <m>m\times n</m> real matrix and <m>\vec{b}\in\R^m</m>. The inconsistent system of equations represented by the matrix equation
<me>
  A\vec{x}=\vec{b}
</me>
has a least squares approximation determined by the solution of the normal equations,
<me>
  (A^TA)\vec{\bar{x}}=A^T\vec{b}.
</me>
</p>

<p>
The residual vector <m>\vec{r}=\vec{b}-A\vec{\bar{x}}</m> is the vector of minimum magnitude in the set
<me>
  \set{\vec{b}-A\vec{x}:\vec{x}\in\R^n}.
</me>
</p>
</statement>
</theorem>
</subsection>

</section>
