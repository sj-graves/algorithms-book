<chapter xmlns:xi="http://www.w3.org/2001/XInclude" xml:id="ch-Vectors">
    <title>Vectors and solving systems of equations</title>


    <objectives>
        <ol type="1">
            <li><p>Mathematically define vectors.</p></li>
            <li><p>Discuss the algebraic operations on vectors.</p></li>
            <li><p>Discuss algorithms for basic vector operations.</p></li>
            <li><p>Implement basic vector operations as a Python subclass.</p></li>
            <li><p>Discuss solutions to systems of linear equations.</p></li>
            <li><p>Introduce Gaussian elimination and Gauss-Jordan elimination.</p></li>
            <li><p>Discuss pivoting strategies and their necessity.</p></li>
            <li><p>Implement elementary row operations and discuss the algorithm for Gaussian elimination with partial pivoting.</p></li>
        </ol>
    </objectives>
    <introduction>
        <title>What is a vector?</title>
        <p>
            The loose definition of a <em>vector</em> from physics and multivariate calculus serve well enough to begin building a notion of the algorithms involving vectors: a vector can be thought of a <em>directed magnitude</em>.
        </p>
        <p>
            What we really need to think of are <em>real vectors</em> which are commonly represented as <m>n\text{-tuples}</m> of real numbers. In keeping with the notation used in Stewart's <em>Essential Calculus</em> textbook, we will denote vectors written in <em>coordinate notation</em> as a list of coordinates enclosed in angle brackets:
            <me>\vec{x} = \vc{x_1, x_2, x_3, \ldots, x_n}.</me>
        </p>
        <p>
            These vectors with <m>n</m> real coordinates can be carefully defined to form <em>Euclidean <m>n</m>-space</em>, which is the extension of the Cartesian plane into <m>  n</m> dimensions.
        </p>
        <p>
            Vectors are very closely related to matrices. In fact, matrices can be thought of as a representation of a particular type of function of a vector space (called a <em>linear transformation</em>). If a matrix <m>A</m> represents a linear transformation <m>\sigma</m>, then the action of the linear transformation on a vector <m>\vec{x}</m> is exactly that of <em>left multiplication of <m>\vec{x}</m> by <m>A</m></em>, when <m>\vec{x}</m> is written as a <em>column vector</em>.
        </p>
        <theorem>
            <statement>
                Let <m>\sigma</m> be a linear transformation and <m>A=[a_{i,j}]_{m\times n}</m> be a matrix associated with <m>\sigma</m>, and let <m>\vec{x}=\vc{x_1,x_2,\ldots,x_n}</m> be a vector. Then
                <md>
                    <mrow>
                        \sigma(\vec{x})  = A\vec{x}
                        <![CDATA[ & = \begin{bmatrix} ]]>
                        <![CDATA[a_{1,1} & a_{1,2} & \cdots & a_{1,n} \\]]>
                        <![CDATA[a_{2,1} & a_{2,2} & \cdots & a_{2,n} \\]]>
                        <![CDATA[\vdots & \vdots & & \vdots \\]]>
                        <![CDATA[a_{m,1} & a_{m,2} & \cdots & a_{m,n} \\]]>
                        \end{bmatrix}
                        \begin{bmatrix}
                        <![CDATA[x_1 \\ x_2 \\ \vdots \\ x_n]]>
                        \end{bmatrix} 
                    </mrow>
                    <mrow>
                        <![CDATA[ & = \begin{bmatrix} ]]>
                        a_{1,1}x_1 + a_{1,2}x_2 + \cdots + a_{1,n}x_n \\
                        a_{2,1}x_1 + a_{2,2}x_2 + \cdots + a_{2,n}x_n \\
                        \vdots \\
                        a_{m,1}x_1 + a_{m,2}x_2 + \cdots + a_{m,n}x_n
                        \end{bmatrix}
                    </mrow>
                </md>
            </statement>
        </theorem>
    </introduction>
    <xi:include  href="sec-vectors-euclidean.xml" />
    <xi:include  href="sec-vectors-ops.xml" />
    <xi:include  href="sec-vectors-motivation.xml" />
    <xi:include  href="sec-vectors-swamping.xml" />
    <xi:include  href="sec-vectors-pivoting.xml" />
    <xi:include  href="sec-vectors-ge.xml" />
    <conclusion xml:id="formal-vectorspace">
        <title>Formal definition of a vector space</title>
        <p> To be very precise, here are the necessary definitions which define a <em>vector space</em>.</p>
        <definition>
            <title>Field</title>
            <statement>
                <p>
                    A <em>field</em> is a set <m>F</m> along with two operations <m>\cdot</m> and <m>+</m>, generally called multiplication and addition, which satisfy the following conditions: suppose <m>f,g,h\in F</m>
                </p>
                <ul>
                <li><em>Closure</em>: <m>f+g \in F</m> and <m>f\cdot g\in F</m>.</li>
                <li><em>Associativity</em>: <m>f+(g+h) = (f+g)+h</m> and <m>f\cdot(g\cdot h) = (f\cdot g)\cdot h</m>.</li>
                <li><em>Commutativity</em>: <m>f+g = g+f</m> and <m>f\cdot g=g\cdot f</m></li>
                <li><em>Identities</em>: there are special elements <m>0\in F</m> and <m>1\in F</m> such that <m>f+0 = f</m> and <m>f\cdot 1=f</m>. These are respectively called the <em>additive identity</em> and the <em>multiplicative identity</em>.</li>
                <li><em>Inverses</em>: for every <m>f\in F</m>, there is a unique element of <m>F</m>, denoted <m>-f</m>, such that <m>f + (-f) = 0</m>. Similarly for each <m>f\neq 0</m> there is a unique element of <m>F</m>, denoted <m>f^{-1}</m>, such that <m>f\cdot f^{-1} = 1</m>.</li>
                <li><em>Distributivity</em>: <m>f\cdot(g + h) = f\cdot g+f\cdot h</m>.</li>
            </ul>
            </statement>
            
        </definition>
        <definition>
            <title>Vector Space</title>
            <statement>
                <p>
                    A set <m>V</m> is a <em>vector space over the field <m>F</m></em> if and only if there are two operations called <em>vector addition</em>, denoted <m>+</m>, and <em>scalar multiplication</em>, denoted <m>\cdot</m>, such that if <m>\vec{u}, \vec{v}, \vec{w}\in V</m> and <m>f,g\in F</m>, then:
                </p>
                <ul>
                    <li><em>Closure</em>: <m>\vec{u}+\vec{v}\in V</m> and <m>f\vec{u}\in V</m>.</li>
                    <li><em>Commutativity of addition</em>: <m>\vec{u}+\vec{v} = \vec{v}+\vec{u}</m>. </li>
                    <li><em>Additive identity</em>: there is a vector <m>\vec{0}\in V</m> such that <m>\vec{u}+\vec{0}=\vec{u}</m>.</li>
                    <li><em>Additive inverses</em>: for each vector <m>\vec{u}\in V</m> there is a unique vector, called the <em>additive inverse of <m>\vec{u}</m> and denoted <m>-\vec{u}</m>, such that <m>\vec{u}+(-\vec{u}) = \vec{0}</m>.</em></li>
                    <li><em>Associativity</em>: <m>f(g\vec{u}) = (fg)\vec{u}</m>.</li>
                    <li><em>Distributivity #1</em>: <m>f(\vec{u}+\vec{v})=f\vec{u}+f\vec{v}</m></li>
                    <li><em>Distributivity #2</em>: <m>(f+g)\vec{u} = f\vec{u}+g\vec{u}</m></li>
                </ul>
            </statement>
        </definition>
    </conclusion>

</chapter>
