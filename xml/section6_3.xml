<section>
<title>Motivation for matrix arithmetic</title>
<p>
Matrices have only been called such<fn>Matrix is latin for ``womb," and here's the explanation for the term from the person who coined it, James Joseph Sylvester: <em>I have in previous papers defined a ``Matrix" as a rectangular array of terms, out of which different systems of determinants may be engendered as from the womb of a common parent.</em></fn> since 1850, but some of the methods for which they have been used have been known in varying parts of the world since as early as the second century BC. Nearly all earliest uses of matrices are for the same purpose: solving systems of simultaneous linear equations.
</p>

<example>
<statement>
<p>
Consider the following system of simultaneous linear equations in three variables:
<men xml:id="eq_sys" >
  \systeme{x_1 + 3x_2 - 13x_3 = -18, x_1 + 2x_2 - 10x_3 = -15, -6x_1 - 7x_2 + 46x_3 = 77}
</men>
</p>

<p>
We can solve this by the method of elimination, using repeated applications of the following three operations:
<ol>
<li><p>The order of equations may be rearranged. For example, the first and third equation could be swapped denoted <m>E_1:E_3</m>.</p></li>
<li><p>An equation may be multiplied by a nonzero constant. For example, the second equation could be multiplied by <m>6</m>, denoted <m>6E_2</m>.</p></li>
<li><p>A nonzero multiple of one equation may be added to another equation. For instance, 3 times the first equation could be added to the second, denoted <m>3E_1+E_2</m></p></li>
</ol>
</p>

<p>
As these operations preserve the arithmetic properties of the equations, the set of solutions before and after any sequence of these operations must be the same. The goal in the method of elimination is to ``diagonalize" the equation, so that the coefficient of <m>x_j</m> in row <m>i</m> is <m>0</m> for all <m>j\lt i</m>. For instance, with our example, we could proceed in the following order:
<md>
  <mrow>  \systeme{x_1 + 3x_2 - 13x_3 = -18, x_1 + 2x_2 - 10x_3 = -15, -6x_1 - 7x_2 + 46x_3 = 77}
	\xrightarrow[6E_1+E_3]{-E_1+E_2}
	\amp \systeme{x_1 + 3x_2 - 13x_3 = -18, {-x_2}+3x_3=3, 11x_2_32x_3=-31}</mrow>
  <mrow>  \nonumber
	\xto{11E_2+E_3}
	\amp \systeme{x_1 + 3x_2 - 13x_3 = -18, {-x_2}+3x_3=3, x_3=2}</mrow>
</md>
</p>

<p>
So eliminated, we can use back substitution to solve for <m>x_3</m>, <m>x_2</m>, and <m>x_1</m> in that order:
<md>
  <mrow>  x_3 \amp = 2</mrow>
  <mrow>  x_2 \amp = \frac1{3}\left(3+x_3\right) = 3</mrow>
  <mrow>  x_1 \amp = -18_3x_2+13x_3 = -1</mrow>
</md>
</p>

<p>
The importance of this method cannot be too highly stressed: solving systems of linear equations is a constant problem in applied mathematics, often because the underlying systems of nonlinear equations can be nicely linearized by making acceptable sacrifices. We notice that at no step were we required to interchange the order of equations, as we never encountered a situation where the <m>i^\text{ th }</m> variable had a coefficient of <m>0</m> in the <m>i^\text{ th }</m> row.
</p>
</statement>
</example>

<p>
The process used in the preceding example has nothing to do with the variables used <mdash /> in fact, they are used solely as placeholders in the computation. Understanding this, we can recast the problem into a vector algebra problem and dispense with the variables entirely.
</p>

<example>
<statement>
<p>
Consider the matrix <m>A</m> and vectors <m>\vec{x}</m> and <m>\vec{b}</m> given by
<md>
  <mrow>  A \amp = \begin{bmatrix}1 \amp  3 \amp  -13</mrow>
  <mrow>  1 \amp  2 \amp  -10</mrow>
  <mrow>  -6 \amp  -7 \amp  46\end{bmatrix} \amp 
\vec{x} \amp = \begin{bmatrix}x_1</mrow>
  <mrow>  x_2</mrow>
  <mrow>  x_3\end{bmatrix} \amp 
\vec{b} \amp = \begin{bmatrix}-18</mrow>
  <mrow>  -15</mrow>
  <mrow>  77\end{bmatrix}.</mrow>
</md>
</p>

<p>
Then the system in <xref ref="eq_sys" autoname="yes" /> is exactly equivalent to the vector equation <m>A\vec{x}=\vec{b}</m>. In order to keep track of the operations performed on both the left and right of the equal sign, it is sufficient to <em>augment</em> the matrix <m>A</m> by the vector <m>\vec{b}</m>, like so:
<md>
  <mrow>  A\vert\vec{b} \amp = 
	\left[\begin{array}{3} 1 \amp  3 \amp  -13 \amp  -18 \\ 1 \amp  2 \amp  -10 \amp  -15\\ -6 \amp  -7 \amp  46 \amp  77
\end{array} \right].</mrow>
</md>
</p>

<p>
Now the three operations of the method of elimination correspond to the <em>elementary row operations</em> on an augmented matrix, and the operations involved in <xref ref="eq_sys_elim" autoname="yes" /> correspond to the following sequence of row operations:
<md>
  <mrow>  \left[\begin{array}{3} 1 \amp  3 \amp  -13 \amp  -18 \\ 1 \amp  2 \amp  -10 \amp  -15\\ -6 \amp  -7 \amp  46 \amp  77
\end{array} \right] 
	\xrightarrow[6R_1+R_3]{-R_1+R_2}
	\amp \left[\begin{array}{3} 1 \amp  3 \amp  -13 \amp  -18 \\ 0 \amp  -1 \amp  3 \amp  3 \\ 0 \amp  11 \amp  -32 \amp  -31
\end{array} \right]</mrow>
  <mrow>  \nonumber
\xto{11R_2+R_3}
	\amp \left[\begin{array}{3} 1 \amp  3 \amp  -13 \amp  -18 \\ 0 \amp  -1 \amp  3 \amp  3 \\ 0 \amp  0 \amp  1 \amp  2
\end{array} \right]</mrow>
</md>
</p>
</statement>
</example>

<p>
This is a <em>row echelon form</em> for the matrix <m>A\vert\vec{b}</m>, and the process of obtaining it is called <em>Gaussian elimination</em>, or informally <em>row reduction</em>. If rows are interchanged or scaled, there can be many distinct row echelon forms of a matrix. However, we could continue to perform row operations until the left-most nonzero entry in each row is a <m>1</m> and it is the only non-zero element in its column. This extended method is called <em>Gauss-Jordan elimination</em>
</p>

<example>
<statement>
<p>
Continuing from <xref ref="eq_aug_elim" autoname="yes" />, here are the final transformations from row echelon form to reduced row echelon form via Gauss-Jordan elimination.
<md>
  <mrow>  \left[\begin{array}{3} 1 \amp  3 \amp  -13 \amp  -18 \\ 0 \amp  -1 \amp  3 \amp  3 \\ 0 \amp  0 \amp  1 \amp  2
\end{array} \right]
	\xto{-R_2}
	\amp \left[\begin{array}{3} 1 \amp  3 \amp  -13 \amp  -18 \\ 0 \amp  1 \amp  -3 \amp  -3 \\ 0 \amp  0 \amp  1 \amp  2
\end{array} \right]
	\xto{-3R_2+R_1}
	\left[\begin{array}{3} 1 \amp  0 \amp  -4 \amp  -9 \\ 0 \amp  1 \amp  -3 \amp  -3 \\ 0 \amp  0 \amp  1 \amp  2
\end{array} \right]</mrow>
  <mrow>  \xrightarrow[3R_3+R_2]{4R_3+R_1}
	\amp \left[\begin{array}{3} 1 \amp  0 \amp  0 \amp  -1 \\ 0 \amp  1 \amp  0 \amp  3 \\ 0 \amp  0 \amp  1 \amp  2
\end{array} \right]</mrow>
</md>
</p>

<p>
Analytically, there is no difference between Gauss-Jordan elimination and regular Gaussian elimination to row echelon form followed by back substitution. However, the computational complexity<fn>Essentially, this is a measurement of the number of operations performed by an algorithm.</fn> of Gauss-Jordan is higher than elimination and substitution.
</p>

<p>
Gauss-Jordan elimination becomes necessary when we desire to compute the <em>inverse of a nonsingular square matrix</em>, later in the course.
</p>
</statement>
</example>
</section>
