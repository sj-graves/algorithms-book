<section xml:id="sec-LUdecomp-Inverses">
    <title>Matrix inverses</title>


    <introduction>
        <p>
            In the last section in <xref ref="thm-ElementaryMatrices" autoname="yes" />, we observed that for certain matrices a sequence of elementary matrices exist which, when multiplied in the correct order, produce the identity matrix. 
        </p>
        <definition>
            <statement>
                <p>
                    Let <m>A</m> be an <m>n\times n</m> matrix. Suppose there is a matrix <m>B</m> such that <m>AB=I</m>. Then <m>B</m> is the <term>inverse</term> of <m>A</m>, and we write <m>A^{-1}</m> instead of <m>B</m>. Moreover, <m>A^{-1}</m> is the unique such matrix and <m>A^{-1}A = I = AA^{-1}</m>. A matrix which has an inverse is an <term>invertible matrix</term>, also called a <term>nonsingular matrix.</term> Similarly, a noninvertible matrix is <term>singular</term>.
                </p>
            </statement>
        </definition>

        <p>
            Combining this definition with the aforementioned theorem, we obtain a powerful result: any matrix <m>A</m> which can be reduced to an identity matrix via row operations is an invertible matrix.
        </p>

        <theorem>
            <statement>
                <p>
                    Suppose <m>A</m> is a <m>n\times n</m> matrix and there is a sequence <m>(E_1,E_2,\ldots,E_k)</m> of elementary matrices such that
                    <me>
                        E_kE_{k-1}\cdots E_2E_1A=I.
                    </me>
                </p>

                <p>
                    Then <m>A^{-1}=E_kE_{k-1}\cdots E_2E_1</m>.
                </p>
            </statement>
        </theorem>
    </introduction>


    <subsection>
        <title>Application of Gauss-Jordan elimination</title>
        <!--p>
            We find the inverse of a matrix by the same process of Gauss-Jordan elimination as used when solving systems of equations, but now we augment our square matrix <m>A</m> by an appropriately sized identity matrix rather than by a single vector.
        </p-->
        <p>
            The full process of <em>Gauss-Jordan elimination</em> now becomes necessary if we wish to easily calculate the inverse of a nonsingular matrix. Since the same process of row operations must be performed on the identity matrix as the matrix <m>A</m> when calculating <m>A^{-1}</m>, it seems more efficient to somehow perform the operations simultaneously. A particularly effective way to do this is to augment the matrix <m>A</m> by the full identity matrix <m>I</m> rather than by as single vector, and then perform the row operations on <m>A|I</m>.
        </p>
        <example>
            <title>Calculation of a matrix inverse</title>
            <statement>
                <p>
                    Suppose we begin with a <m>3\times 3</m> matrix <m>A</m> augmented by <m>I</m>:
                    <me>
                        A|I = \left[\begin{array}{rrr|rrr}
                        1 \amp  3 \amp  5 \amp  1 \amp  0 \amp  0 \\
                        7 \amp  9 \amp  11 \amp  0 \amp  1 \amp  0 \\
                        2 \amp  -4 \amp  -6 \amp  0 \amp  0 \amp  1
                        \end{array}\right].
                    </me>
                </p>

                <p>
                    We know that many different sequences of row operations can result in identical reduced row echelon forms; hence we will apply Gaussian elimination with partial pivoting. When working a problem like this, it is often useful to combine multiple row operations on one arrow. When more than one operation occurs on an arrow in this example, they are being performed in the order from top to bottom.
                </p>
                <md>
                    <mrow>  
                        \left[\begin{array}{rrr|rrr}
                        <![CDATA[1 & 3 & 5 & 1 & 0 & 0 \\]]>
                        <![CDATA[ \fbox{$7$} & 9 & 11 & 0 & 1 & 0 \\ ]]>
                        <![CDATA[ 2 & -4 & -6 & 0 & 0 & 1 ]]>
                        \end{array}\right]

                        \xrightarrow[\frac17 R_1]{R_1:R_2}
                        \amp 

                        \left[\begin{array}{rrr|rrr}
                        <![CDATA[1 & \frac97 & \frac{11}7 & 0 & \frac17 & 0 \\]]>
                        <![CDATA[1 & 3 & 5 & 1 & 0 & 0 \\]]>
                        <![CDATA[2 & -4 & -6 & 0 & 0 & 1 ]]>
                        \end{array}\right]
                    </mrow>
                    <mrow>
                        \xrightarrow[-2R_1+R_3]{-R_1+R_2} \amp
                        \left[\begin{array}{rrr|rrr}
                        <![CDATA[1 & \frac97 & \frac{11}7 & 0 & \frac17 & 0 \\ ]]>
                        <![CDATA[0 & \frac{12}7 & \frac{24}7 & 1 & -\frac17 & 0 \\]]>
                        <![CDATA[0 & \fbox{$-\frac{46}7$} & -\frac{64}7 & 0 & -\frac27 & 1]]>
                        \end{array}\right]
                    </mrow>
                    <mrow>
                        \xrightarrow[-\frac7{46}R_2]{R_2:R_3} \amp
                        \left[\begin{array}{rrr|rrr}
                        <![CDATA[1 & \frac97 & \frac{11}7 & 0 & \frac17 & 0 \\ ]]>
                        <![CDATA[0 & 1 & \frac{32}{23} & 0 & \frac1{23} & -\frac7{46} \\]]>
                        <![CDATA[0 & \frac{12}7 & \frac{24}7 & 1 & -\frac17 & 0]]>
                        \end{array}\right]
                    </mrow>
                    <mrow>
                        \xrightarrow[-\frac{12}7R_2+R_3]{-\frac97R_2+R_1} \amp
                        \left[\begin{array}{rrr|rrr}
                        <![CDATA[1 & 0 & -\frac5{23} & 0 & \frac2{23} & \frac9{46} \\]]>
                        <![CDATA[0 & 1 & \frac{32}{23} & 0 & \frac1{23} & -\frac7{46} \\]]>
                        <![CDATA[0 & 0 & \fbox{$\frac{24}{23}$} & 1 & -\frac5{23} & \frac{6}{23}]]>
                        \end{array}\right]
                    </mrow>
                    <mrow>
                        \xrightarrow[\begin{matrix}\frac5{23}R_3+R_1 \\ -\frac{32}{23}R_3+R_2 \end{matrix}]{\frac{23}{24}R_3} \amp
                        \left[\begin{array}{rrr|rrr}
                        <![CDATA[1 & 0 & 0 & \frac5{24} & \frac1{24} & \frac14 \\]]>
                        <![CDATA[0 & 1 & 0 & -\frac43 & \frac13 & -\frac12 \\]]>
                        <![CDATA[0 & 0 & 1 & \frac{23}{24} & -\frac5{24} & \frac14]]>
                        \end{array}\right]
                    </mrow>
                </md>


                <p>
                    You are encouraged to check that
                    <me>
                        \left[\begin{array}{rrr}
                        <![CDATA[ 1 &  3 &  5 \\]]>
                        <![CDATA[ 7 &  9 & 11 \\ ]]>
                        <![CDATA[ 2 & -4 & -6 ]]>
                        \end{array}\right]
                        \left[\begin{array}{rrr}
                        <![CDATA[\frac5{24} & \frac1{24} & \frac14 \\]]>
                        <![CDATA[-\frac43 & \frac13 & -\frac12 \\]]>
                        <![CDATA[\frac{23}{24} & -\frac5{24} & \frac14]]>
                        \end{array}\right] = I
                    </me>.
                </p>
            </statement>
        </example>
        <exercise>
            <title>Implementing matrix inverses</title>
            <statement>
                Extend your thinking! Try to adapt your Gaussian elimination with partial pivoting algorithm to calculate matrix inverses.
            </statement>
        </exercise>
    </subsection>
    <subsection>
        <title>Application to solving systems</title>
        A wonderful algebraic use for a matrix inverse comes into play when solving systems of equations. For instance, if <m>A</m> is an invertible matrix, then
        <me>
            A\vec{x} = \vec{b} \Longleftrightarrow \vec{x} = A^{-1}\vec{b}.
        </me>
        This is useful especially when there are multiple <em>loading vectors</em>, <m>\vec{b}_1, \vec{b}_2, \ldots</m>, and it is necessary to solve <m>A\vec{x}=\vec{b}_i</m> for each of them. While this is a very beautiful application of the inverse, it can be shown that using matrix multiplication in this way over a large set of loading vectors becomes less efficient than other methods, notably the <m>LU</m> decomposition and the <m>PA=LU</m> factorizations.
    </subsection>

</section>
