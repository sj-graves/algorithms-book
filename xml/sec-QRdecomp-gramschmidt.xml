<section xml:id="sec-QRdecomp-GramSchmidt">
    <title>Gram-Schmidt orthonormalization</title>
    <p>
        A first method for producing an orthogonal matrix from an arbitrary matrix is the Gram-Schmidt algorithm. In essence, the idea is to treat each column of the initial matrix as a vector, and proceed through in order orthogonalizing each vector to those which have previously been orthonormalized.
    </p>
    <subsection>
        <title>An example with two vectors</title>
        <p>
            Consider the two vectors
            <md>
                <mrow>
                    \vec{x}_1 \amp = \vc{1,1,1,1,1} \amp \vec{x}_2 \amp = \vc{-2,-1,0,1,3}.
                </mrow>
            </md>
            Since <m>\vec{x}_1\neq \lambda \vec{x}_2</m> for any scalar <m>\lambda</m>, there is a unique plane in <m>\R^5</m> containing both <m>\vec{x}_1</m> and <m>\vec{x}_2</m>. Our goal will be to produce a pair of vectors <m>\vec{q}_1</m> and <m>\vec{q}_2</m> such that
            <md>
                <mrow>
                    \abs{\vec{q}_1} \amp = 1 = \abs{\vec{q}_2} 
                </mrow>
                <intertext>and</intertext>
                <mrow>
                    \vec{q}_1\cdot\vec{q}_2 \amp = 0, 
                </mrow>
            </md>
            and moreover that <m>\vec{q}_1</m> be some scalar multiple of <m>\vec{x}_1</m>. This immediately tells us that <m>\vec{q}_1 = \frac{\vec{x}_1}{\abs{\vec{x}_1}}</m>. Consider the following diagram regarding the calculation of <m>\vec{q}_2</m>.
        </p>
        <figure>
            <image source="./images/orthogonal_vectors.svg" width="50%"/>
            <caption>Two vectors <m>\vec{x}_1</m> and <m>\vec{x}_2</m> in the plane and the derived pair of orthonormal vectors <m>\vec{q}_1</m> and <m>\vec{q}_2</m>.</caption>
        </figure>
        <p>
            We can geometrically construct the perpendicular line to the vector <m>\vec{x}_1</m> through the head of the vector <m>\vec{x}_2</m> with ease; the unit vector parallel to that line with tail at the origin and head on the same side of <m>\vec{x}_1</m> as the head of <m>\vec{x}_2</m>. Algebraically, it's a little bit more complicated. Let's start off by saying the vector pointing to the intersection of this perpendicular with <m>\vec{x}_1</m> is <m>\text{proj}_{\vec{x}_1}\vec{x}_2 = \lambda \vec{q}_1</m>. Then we know that the vector orthorgonal to <m>\vec{q}_1</m> on the same side of <m>\vec{x}_1</m> as <m>\vec{x}_2</m> must be <m>\vec{w} = \vec{x}_2 - \text{proj}_{\vec{q}_1}\vec{x}_2</m> so that 
            <me>
                \vec{w}+\text{proj}_{\vec{q}_1}\vec{x}_2 = \vec{x}_2.
            </me>
            Now, since we must have <m>\vec{w}\cdot\vec{q}_1 = 0,</m> we see
            <md>
                <mrow>
                    0 \amp = (\vec{x}_2 - \text{proj}_{\vec{q}_1}\vec{x}_2)\cdot \vec{q}_1
                </mrow>
                <mrow>
                    \amp = \vec{q}_1\cdot\vec{x_2} - \vec{q}_1\cdot \lambda\vec{q}_1
                </mrow>
                <intertext>and equivalently</intertext>
                <mrow>
                    \lambda = \frac{\vec{q}_1\cdot\vec{x}_2}{\vec{q}_1\cdot\vec{q}_1}.
                </mrow>
            </md>
            Since <m>\abs{\vec{q}_1} = 1,</m> this simplifies our calculations to <m>\vec{q}_2 = \vec{x}_2 - (\vec{q}_1\cdot\vec{x_2})\vec{q}_1</m>.
        </p>
    </subsection>
    <subsection>
        <title>Extending to more than two vectors</title>
        <p>
            The process delineated above for orthonormalizing a set of two vectors extends naturally to an arbitrary set of <m>k\leq n</m> vectors in <m>\R^n</m>, by considering the following: to construct the orthnormalized <m>\vec{q}_i</m> from the orthonormalized <m>\vec{q}_j</m> for <m>j\lt i</m> and the given vector <m>\vec{x}_i</m> by iteratively subtracting from <m>\vec{x}_i</m> the projections of <m>\vec{x}_i</m> onto each of the vectors <m>\vec{q}_j</m>.
        </p>
        <definition>
            <statement>
                The set <m>B = \set{\vec{b}_1,\vec{b}_2,\ldots,\vec{b}_n}</m> is a <term>basis for <m>\R^n</m></term> if and only if <m>B</m> is linearly independent and the span of <m>B</m> is all of <m>\R^n</m>. The set is an <term>orthonormal basis</term> for <m>\R^n</m> if and only if each vector has <m>\abs{\vec{b}_i }= 1</m> and if for each <m>i\neq j</m>, <m>\vec{b}_i\cdot\vec{b}_j = 0</m>.
            </statement>
        </definition>
        <p>
            By this definition, we see that any set of <m>k\lt n</m> linearly independent vectors can be orthonormalized; a set of <m>n</m> linearly independent vectors in <m>\R^n</m> will necessarily produce via orthonormalization an orthonormal basis for <m>\R^n</m>. The procedure we have just described is called <term>Gram-Schmidt orthonormalization</term>.
        </p>
        <theorem>
            <title>Gram-Schmidt Orthonormalization</title>
            <statement>
                <p>
                    Suppose <m>S=\set{\vec{x}_1,\vec{x}_2,\cdots,\vec{x}_n}</m> is a set of vectors in <m>\R^m</m>. Suppose
                    <md>
                        <mrow>  \vec{w}_1 \amp = \vec{x}_1,\text{ and }</mrow>
                        <mrow>  \vec{q}_1 \amp = \vec{w}_1/\abs{\vec{w}_1}.</mrow>
                    </md>
                </p>

                <p>
                    Then for each <m>i=2,3,\ldots,n</m>, let
                    <md>
                        <mrow>  \vec{w}_i \amp = \vec{x}_i-\sum_{j=1}^{i-1} (\vec{q}_j\cdot\vec{x}_i)\vec{q}_j,\text{ and }</mrow>
                        <mrow>  \vec{q}_i \amp = \vec{w}_i/\abs{\vec{w}_i}.</mrow>
                    </md>
                </p>

                <p>
                    Then <m>Q=\set{\vec{q}_1,\vec{q}_2,\ldots,\vec{q}_n}</m> is an orthonormal basis for <m>\lspan(S)</m>.
                </p>
            </statement>
        </theorem>
        <proof>
            <p>
                Suppose <m>1\leq k\lt \ell\leq n</m>. Then
                <md>
                    <mrow>  \vec{q}_k\cdot\vec{w}_{\ell} \amp = \vec{q}_k\cdot\vec{x}_{\ell} - \vec{q}_k\cdot\sum_{j=1}^{\ell-1} (\vec{q}_j\cdot\vec{x}_\ell)\vec{q}_j</mrow>
                    <mrow>  \amp = \vec{q}_k\cdot\vec{x}_{\ell} - \sum_{j=1}^{\ell-1}(\vec{q}_j\cdot \vec{x}_\ell)(\vec{q}_k\cdot\vec{q}_j)</mrow>
                </md>
            </p>

            <p>
                Since
                <md>
                    <mrow>  \vec{q}_k\cdot\vec{q}_j \amp = \begin{cases}0\amp k\neq j\\1\amp k=j,
                        \end{cases}</mrow>
                </md>
                we have <m>\vec{q}_k\cdot\vec{w}_\ell = \vec{q}_k\cdot\vec{x}_\ell - \vec{q}_k\cdot\vec{x}_\ell = 0</m>. Thus <m>\vec{q}_k\cdot\vec{q}_\ell = \frac1{\abs{\vec{w}_\ell}}(\vec{q}_k\cdot\vec{w}_\ell) = 0</m> and <m>\vec{q}_k</m> is orthogonal to <m>\vec{q}_{\ell}</m> whenever <m>k\lt \ell</m>. Moreover,
                <me>
                    \abs{\vec{q}_k} = \abs{\frac{\vec{w}_k}{\abs{\vec{w}_k}}} = 1,
                </me>
                so <m>Q</m> is a set of mutually orthogonal unit vectors. Since <m>\vec{w}_\ell=\abs{\vec{w}_\ell}\vec{q}_\ell</m>, each vector <m>\vec{x}_\ell</m> can be written as a linear combination from <m>\set{\vec{q}_k:k\leq\ell}</m>; hence any linear combination of vectors in <m>S</m> can be written as a linear combination of vectors in <m>Q</m>, and vice versa. Thus <m>\lspan S=\lspan Q</m> and the result holds.
            </p>
        </proof>
        <p>
            As written there is a sizeable numerical instability to the algorithm of Gram-Schmidt orthonormalization: the calculation 
            <md>
                <mrow>
                    \vec{w}_i \amp = \vec{x}_i-\sum_{j=1}^{i-1} (\vec{q}_j\cdot\vec{x}_i)\vec{q}_j
                </mrow>
            </md>
            in the <m>i^\text{th}</m> iteration grows wildly inaccurate in floating point arithmetic. A minor modification produces the <term>modified Gram-Schmidt orthonormalization</term>.
        </p>
    </subsection>


    <algorithm xml:id='alg-MGSO'>
        <title>Modified Gram-Schmidt Orthonormalization</title>
        <statement>
            <p>
                Let
                <me>
                    S=\set{\vec{x}_j:1\leq j\leq n}
                </me>
                be a set of vectors in <m>\R^m</m>.
            </p>

            <ol>
                <li><p>For each <m>j</m> in <m>\set{1,2,\ldots,n}</m>, set <m>\vec{w}_j = \vec{x}_j</m>.</p></li>
                <li><p>For <m>j</m> in <m>\set{1,2,\ldots,n}</m>, do the following.
                    </p>

                    <ol>
                        <li><p>For each <m>i\lt j</m>, set <m>\vec{w}_j = \vec{w}_j - (\vec{q_i}\cdot \vec{w}_j)\vec{q}_i</m>.</p></li>
                        <li><p>Let <m>\vec{q}_j = \frac1{\abs{\vec{w}_j}}\vec{w}_j</m>.</p></li>
                    </ol>
                </li>
            </ol>

            <p>
                The set <m>Q=\set{\vec{q}_j:1\leq j\leq n}</m> is an orthonormal basis for <m>\lspan(S)</m>.
            </p>
        </statement>
    </algorithm>
</section>
