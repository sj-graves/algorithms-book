<section xml:id="sec-QRdecomp-QRdecomp">
    <title>QR decomposition</title>

    <introduction>
        <p>
            With the tool of the Modified Gram-Schmidt algorithm in hand, we have everything necessary to produce a <m>QR</m> decomposition. 
        </p>
    </introduction>
    <subsection>
        <title>Reduced <m>QR</m> decomposition</title>
        <p>
            We begin with an <m>m\times n</m> matrix <m>A</m>, where <m>m\geq n</m>, which we think of as a matrix of <m>n</m> column vectors <m>\vec{x}_1, \vec{x}_2, \dotsc, \vec{x}_n\in \R^m</m>. If these vectors are linearly independent<fn>This can be checked in a number of ways, not the least of which is to perform Gaussian elimination on <m>A</m>; if the resulting matrix is upper triangular, then the column vectors of <m>A</m> are linearly independent.</fn>, they can be orthonormalized into a set of vectors <m>\set{\vec{q}_1,\dotsc,\vec{q}_n}</m> via Gram-Schmidt. Collecting those vectors as the columns of a matrix <m>Q</m>, we have the first factor of the <m>QR</m> decomposition.
        </p>
        <p>
            The matrix <m>R</m> is an upper triangular matrix, whose entries depend upon calculations performed in <xref ref="alg-MGSO" autoname="title" />. Specifically, if we write <m>R=[r_{i,j}]_{n\times n}</m>, then 
            <me>
                r_{i,j} = \begin{cases}
                \vec{q}_i\cdot\vec{w}_j,    \amp j > i \\
                \abs{\vec{w}_j},            \amp j = i \\
                0,                          \amp j \lt i.
                \end{cases}
            </me>
        </p>
        <p>
            The choice of components for the matrix <m>R</m> serves precisely to ensure that 
            <me>
                a_{i,j} = \sum_{k=1}^n q_{i,k} r_{k,j}
            </me>
            since <m>a_{i,j}</m> is the <m>j^\text{th}</m> component of the vector <m>\vec{x}_i</m> written as a column of <m>A</m>.
            Hence <m>A=QR</m> by construction. We're a small step away from the production of our first orthogonal matrix, and the only thing stopping us is that <m>A</m> is likely a non-square matrix.
        </p>
    </subsection>
    <subsection>
        <title>Full <m>QR</m> decomposition</title>
        <p>
            When <m>A=[a_{i,j}]_{m\times n}</m> and <m>n \lt m</m>, the matrix <m>Q</m> produced via the reduced <m>QR</m> decomposition cannot possibly be orthogonal, as only square matrices are invertible. In order to create from <m>A</m> an orthogonal matrix, let <m>B=[b_{i,j}]_{m\times(m-n)}</m> be a matrix whose columns are linearly independent from each other and from the columns of <m>A</m>. The process of <m>QR</m> decomposition described above, performed upon the augmented matrix <m>A|B</m>, produces factors <m>Q</m> and <m>R</m> such that
            <me>
                A = I_{m\times n}(A|B) = QR,
            </me>
            where <m>I_{m\times n}</m> is the matrix consisting of the first <m>n</m> columns of the identity matrix in dimension <m>n</m>.
        </p>
        <p>
            Even with this addition, it is important to note that the resulting matrices <m>Q</m> and <m>R</m> are not necessarily unique unless other properties are specified.
        </p>
    </subsection>
    <example>
        <title>A complete <m>QR</m> decomposition example</title>
        <p>
            This example is <em>extremely</em> long and detailed. Suppose we wish to compute a full <m>QR</m> decomposition of the matrix 
            <me>
                A = <!--[[1,0,-1],[1,2,-2],[-2,1,1],[0,0,1]]-->
                \left[\begin{array}{rrr}
                1 \amp 0 \amp -1 \\
                1 \amp 2 \amp -2 \\
                -2\amp 2 \amp 1  \\
                0 \amp 0 \amp 1
                \end{array}\right].
            </me>
            We begin by choosing a matrix by which to augment; it is often effective to choose the correct number of columns of an identity matrix. Our result is the augmented matrix
            <me>
                A|B = 
                \left[\begin{array}{rrr|r}
                1 \amp 0 \amp -1 \amp 1 \\
                1 \amp 2 \amp -2 \amp 0 \\
                -2\amp 2 \amp 1  \amp 0 \\
                0 \amp 0 \amp 1  \amp 0
                \end{array}\right],
            </me>
            and our corresponding vectors are 
            <md>
                <mrow>
                    \vec{x}_1 \amp = \vc{1,1,-2,0}, \amp
                    \vec{x}_2 \amp = \vc{0,2,2,0},
                </mrow>
                <mrow>
                    \vec{x}_3 \amp = \vc{-1,-2,1,1}, \amp
                    \vec{x}_4 \amp = \vc{1,0,0,0}.
                </mrow>
            </md>
            Initializing <m>\vec{w}_i=\vec{x}_i</m> for <m>i=1,2,3,4</m>, we immediately obtain <m>\vec{q}_1</m> by
            <md>
                <mrow>
                    \abs{\vec{w}_1} \amp = \sqrt{1^2+1^2+(-2)^2+0^2} = \sqrt{6} \to r_{1,1}
                </mrow>
                <intertext>and hence</intertext>
                <mrow>
                    \vec{q}_1 \amp = \frac{\vec{w}_1}{\abs{\vec{w}_1}} = \frac{1}{\sqrt{6}}\vc{1, 1, -2, 0}.
                </mrow>
            </md>
        </p>
        <p>
            As <m>\vec{q}_1</m> is a unit vector, we next update our current <m>\vec{w}_2</m>, by subtracting from it <m>(\vec{q}_1\cdot\vec{w_2})\vec{q}_1</m>,
            noting that <m>(\vec{q}_1\cdot\vec{w_2}) = \left(0+\frac2{\sqrt{6}}-\frac4{\sqrt{6}}+0\right) = -\frac2{\sqrt{6}}\to r_{1,2}.</m>
            <md>
                <mrow>
                    \vec{w}_2 - (\vec{q}_1\cdot\vec{w_2})\vec{q}_1 
                    \amp = \vc{0,2,2,0} + \frac2{\sqrt{6}} \cdot\frac{1}{\sqrt{6}}\vc{1,1,-2,0} 
                </mrow>
                <mrow>
                    \amp = \vc{0,2,2,0}+\frac26\vc{1,1,-2,0} 
                </mrow>
                <mrow>
                    \amp = \frac13\vc{1,7,4,0} \to \vec{w}_2.
                </mrow>
            </md>
            Since we have no orthogonalized <m>\vec{w}_2</m> to <m>\vec{q}_i</m> for all <m>i\lt 2</m>, we must now normalize <m>\vec{w}_2</m>. Since
            <md>
                <mrow>
                    \abs{\vec{w}_2} \amp = \frac13\sqrt{1^2+7^2+4^2+0^2} = \frac{\sqrt{66}}3 \to r_{2,2} 
                </mrow>
            </md>
            we get
            <me>
                \vec{q}_2 = \frac{\vec{w}_2}{\abs{\vec{w}_2}} = \frac3{\sqrt{66}}\cdot\frac13\vc{1,7,4,0} = \frac{1}{\sqrt{66}}\vc{1,7,4,0}.
            </me>
        </p>
        <p>
            We next update <m>\vec{w}_3</m> using <m>\vec{q}_1</m> and <m>\vec{q}_2</m> to produce <m>\vec{q}_3</m>. We first calculate 
            <md>
                <mrow>
                    \vec{q}_1\cdot\vec{w}_3 
                    \amp = 
                    \frac1{\sqrt{6}}\vc{1,1,-2,0}\cdot\vc{-1,-2,1,1}
                </mrow>
                <mrow>
                    \amp = -\frac1{\sqrt{6}}(1(-1)+1(-2)-2(1)+0(1)) = -\frac5{\sqrt{6}} \to r_{1,3}.
                </mrow>
                <intertext>which gives</intertext>
                <mrow>
                    \vec{w}_3 - (\vec{q}_1\cdot\vec{w}_3)\vec{q}_1 
                    \amp = \vc{-1,-2,1,1} + \frac{5}{\sqrt{6}}\cdot\frac1{\sqrt{6}}\vc{1,1,-2,0}
                </mrow>
                <mrow>
                    \amp = \vc{-1,-2,1,1} + \frac56 \vc{1,1,-2,0}
                </mrow>
                <mrow>
                    \amp = \frac16\vc{-1,-7,-4,6} \to \vec{w}_3.
                </mrow>
            </md>
            This updated vector <m>\vec{w}_3</m> is now orthogonal to <m>\vec{q}_1</m> but not yet to <m>\vec{q}_2</m>. Hence we determine
            <md>
                <mrow>
                    \vec{q}_2\cdot \vec{w}_3 \amp = \frac1{\sqrt{66}}\vc{1,7,4,0}\cdot\frac16\vc{-1,-7,-4,6}
                </mrow>
                <mrow>
                    \amp = \frac1{6\sqrt{66}}(1(-1)+7(-7)+4(-4)+0(6)) = -\frac{66}{6\sqrt{66}} = -\frac{\sqrt{66}}6\to r_{2,3}.
                </mrow>
            </md>
            Again updating <m>\vec{w}_3</m> we obtain
            <md>
                <mrow>
                    \vec{w}_3 - (\vec{q}_2\cdot\vec{w}_3)\vec{q}_2
                    \amp = \frac16\vc{-1,-7,-4,6} + \frac{\sqrt{66}}6\cdot\frac{1}{\sqrt{66}}\vc{1,7,4,0}
                </mrow>
                <mrow>
                    \amp = \frac16\vc{-1,-7,-4,6} + \frac16\vc{1,7,4,0}
                </mrow>
                <mrow>
                    \amp = \vc{0,0,0,1} \to \vec{w}_3.
                </mrow>
            </md>
            In this case, it should be readily apparent that <m>\abs{\vec{w_3}}=1\to r_{3,3}</m>, which easily gives
            <me>
                \vec{q}_3 = \vc{0,0,0,1}.
            </me>
        </p>
        <p>
            The final vector remains, and we must first update <m>\vec{w}_4</m> three times before it is orthogonal to each of <m>\vec{q}_1</m>, <m>\vec{q}_2</m>, and <m>\vec{q}_3</m>. Starting with
            <md>
                <mrow>
                    \vec{q}_1\cdot\vec{w}_4 
                    \amp = \frac{1}{\sqrt{6}}\vc{1, 1, -2, 0} \cdot \vc{1,0,0,0}
                </mrow>
                <mrow>
                    \amp = \frac{1}{\sqrt{6}}(1(1)+1(0)-2(0)+0(0)) = \frac{1}{\sqrt{6}} \to r_{1,4},
                </mrow>
            </md>
            we first update <m>\vec{w}_4</m> by
            <md>
                <mrow>
                    \vec{w}_4 - (\vec{q}_1\cdot\vec{w}_4)\vec{q}_1
                    \amp = \vc{1,0,0,0} - \frac{1}{\sqrt{6}} \cdot \frac{1}{\sqrt{6}}\vc{1, 1, -2, 0}
                </mrow>
                <mrow>
                    \amp = \vc{1,0,0,0} - \frac16 \vc{1, 1, -2, 0}
                </mrow>
                <mrow>
                    \amp = \frac16\vc{5,-1,2,0} \to \vec{w}_4.
                </mrow>
            </md>
            Next,
            <md>
                <mrow>
                    \vec{q}_2\cdot\vec{w}_4
                    \amp = \frac{1}{\sqrt{66}}\vc{1,7,4,0} \cdot \frac16\vc{5,-1,2,0}
                </mrow>
                <mrow>
                    \amp = \frac{1}{6\sqrt{66}}(1(5)+7(-1)+4(2)+0(0)) = \frac6{6\sqrt{66}} = \frac1{\sqrt{66}} \to r_{2,4}
                </mrow>
            </md>
            gives us
            <md>
                <mrow>
                    \vec{w}_4 - (\vec{q}_2\cdot\vec{w}_4)\vec{q}_2
                    \amp = \frac16\vc{5,-1,2,0} - \frac1{\sqrt{66}}\cdot \frac{1}{\sqrt{66}}\vc{1,7,4,0}
                </mrow>
                <mrow>
                    \amp = \frac{11}{66}\vc{5,-1,2,0} - \frac{1}{66}\vc{1,7,4,0}
                </mrow>
                <mrow>
                    \amp = \frac{18}{66}\vc{3,-1,1,0} \to \vec{w}_4.
                </mrow>
            </md>
            To orthogonalize <m>\vec{w}_4</m> to <m>\vec{q}_3</m> turns out to be easy, since
            <md>
                <mrow>
                    \vec{q}_3\cdot\vec{w}_4 
                    \amp = \vc{0,0,0,1}\cdot \frac{18}{66}\vc{3,-1,1,0}
                </mrow>
                <mrow>
                    \amp = \frac{3}{11}(0(3)+0(-1)+0(1)+1(0)) = 0 \to r_{3,4};
                </mrow>
            </md>
            that is to say, <m>\vec{w}_4</m> is already orthogonal to <m>\vec{q}_3</m> and we need subtract nothing from <m>\vec{w}_3</m> in this step. Therefore since
            <md>
                <mrow>
                    \abs{\vec{w}_4} \amp = \frac3{11}\sqrt{3^2+1^2+1^2+0^2} = \frac{3\sqrt{11}}{11} = \frac3{\sqrt{11}} \to r_{4,4},
                </mrow>
                <intertext>our final vector is</intertext>
                <mrow>
                    \vec{q}_4 \amp = \frac{\vec{w}_4}{\abs{\vec{w}_4}} = \frac{\sqrt{11}}3\cdot \frac{3}{11}\vc{3,-1,1,0}
                </mrow>
                <mrow>
                    \amp = \frac1{\sqrt{11}}\vc{3,-1,1,0}.
                </mrow>
            </md>
        </p>
        <p>
            Paying careful attention to the assignments in the above process to the various elements <m>r_{i,j}</m> of the matrix <m>R</m>, we have produced
            <md>
                <mrow>
                    Q \amp = \left[\begin{array}{rrr|r}
                    1/\sqrt{6} \amp 1/\sqrt{66} \amp 0 \amp 3/\sqrt{11} \\
                    1/\sqrt{6} \amp 7/\sqrt{66} \amp 0 \amp -1/\sqrt{11} \\
                    -2/\sqrt{6} \amp 4/\sqrt{66} \amp 0 \amp 1/\sqrt{11} \\
                    0 \amp 0 \amp 1 \amp 0
                    \end{array}\right] 
                </mrow>
                <mrow>
                    \amp = 
                    \frac1{\sqrt{66}}\left[\begin{array}{rrr|r}
                    \sqrt{11} \amp 1 \amp 0 \amp 3\sqrt{6} \\
                    \sqrt{11} \amp 7 \amp 0 \amp -\sqrt{6} \\
                    -2\sqrt{11} \amp 4 \amp 0 \amp \sqrt{6} \\
                    0 \amp 0 \amp \sqrt{66} \amp 0
                    \end{array}\right]
                </mrow>
                <intertext>and</intertext>
                <mrow>
                    R \amp = \left[\begin{array}{rrrr}
                    \sqrt{6} \amp -2/\sqrt{6} \amp -5/\sqrt{6} \amp 1/\sqrt{6} \\
                    0 \amp \frac13{\sqrt{66}} \amp -\frac16\sqrt{66} \amp 1/\sqrt{66} \\
                    0 \amp 0 \amp 1 \amp 0 \\
                    0 \amp 0 \amp 0 \amp 3/\sqrt{11}
                    \end{array}\right].
                </mrow>
            </md>
        </p>
        <p>
            If the length of computation for this <q>simple</q> example with three vectors from <m>\R^4</m> is not sufficient to demonstrate the necessity for solving the problem with a computer, nothing is! Moreover, this example was specially chosen so that it was numerically well-behaved.
        </p>
    </example>

    <subsection>
        <title>An application of <m>QR</m> decomposition: Least Squares Regression</title>
        <p>
            While we can use the <m>QR</m> decomposition to solve a system of equations, it is approximately <em>three times more complicated</em> than <m>LU</m> decomposition. However, <m>QR</m> decomposition can avoid a problem called <em>ill-conditioning</em> in solving the least squares problem. Ill-conditioning, very loosely speaking, measures how much a small change in input will be amplified in the output, and in the case of solving a matrix equation is dependent only upon the matrix, not the algorithm used to solve the equation.
        </p>
        <algorithm>
            <title>Least squares by <m>QR</m> decomposition</title>
            <statement>
                <p>
                    Given the <m>m\times n</m> inconsistent system of equations represented by <m>A\vec{x}=\vec{b}</m>, let <m>QR=A</m> be the full <m>QR</m> decomposition of <m>A</m> and let
                    <md>
                        <mrow>  \hat{R} \amp = \text{ upper \(n\times n\) submatrix of \(R\) }</mrow>
                        <mrow>  \hat{\vec{d}} \amp = \text{ upper \(n\) entries of } \vec{d}=Q^T\vec{b}</mrow>
                    </md>
                </p>

                <p>
                    Then the solution to <m>\hat{R}\vec{\bar{x}}=\hat{\vec{d}}</m> is the least squares solution <m>\bar{\vec{x}}</m>.
                </p>
            </statement>
        </algorithm>
    </subsection>
</section>
